{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the Data</h2>\n",
    "\n",
    "Load the cleaned data and split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm\n",
    "\n",
    "train = pd.read_csv('data/train_cleaned.csv')\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train['clean_text'], train['target'], test_size=0.2, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LightGBM</h2>\n",
    "\n",
    "First, we have to featurize our text. I'll use TfidfVectorizer for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "train_data_features = tfidf.fit_transform(X_train)\n",
    "val_data_features = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the data transformation required for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lightgbm.Dataset(train_data_features, label=y_train)\n",
    "val_set = lightgbm.Dataset(val_data_features, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.03,\n",
    "    'boosting_type': 'gbdt', # gradient boosting decision tree\n",
    "    'objective': 'binary', # binary classifiction problem\n",
    "    'metric': 'binary_logloss', # for binary classification\n",
    "    'num_leaves': 20, # number of leaves in a full tree\n",
    "    'min_data': 50, # minimum number of records a leaf may have. default = 20\n",
    "    #'max_depth': 10 # maximum depth of tree; decrease if overfitting\n",
    "}\n",
    "\n",
    "lgb_model = lightgbm.train(params, train_set, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.3 is 0.5843729481286933\n",
      "F1 score at threshold 0.31 is 0.5869993434011819\n",
      "F1 score at threshold 0.32 is 0.5896257386736704\n",
      "F1 score at threshold 0.33 is 0.5935653315824031\n",
      "F1 score at threshold 0.34 is 0.706500328299409\n",
      "F1 score at threshold 0.35 is 0.7091267235718975\n",
      "F1 score at threshold 0.36 is 0.7097833223900197\n",
      "F1 score at threshold 0.37 is 0.7124097176625082\n",
      "F1 score at threshold 0.38 is 0.7130663164806303\n",
      "F1 score at threshold 0.39 is 0.7137229152987524\n",
      "F1 score at threshold 0.4 is 0.7124097176625082\n",
      "F1 score at threshold 0.41 is 0.7156927117531189\n",
      "F1 score at threshold 0.42 is 0.716349310571241\n",
      "F1 score at threshold 0.43 is 0.7202889034799738\n",
      "F1 score at threshold 0.44 is 0.7222586999343401\n",
      "F1 score at threshold 0.45 is 0.7229152987524623\n",
      "F1 score at threshold 0.46 is 0.721602101116218\n",
      "F1 score at threshold 0.47 is 0.7222586999343401\n",
      "F1 score at threshold 0.48 is 0.7222586999343401\n",
      "F1 score at threshold 0.49 is 0.7242284963887065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "lgb_probs = lgb_model.predict(val_data_features, 500)\n",
    "\n",
    "for thresh in np.arange(0.3, 0.5, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, accuracy_score(y_val, (lgb_probs>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears, that the threshold for classifying as non-disaster vs disaster tweet is 0.45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229152987524623"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_val== (lgb_probs>0.45)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good of a performance as the logistic regression model.  Perhaps, a blend of the two will improve performance since the two models are pretty different and they may have learned differently and can combine to give a better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Blended Logistic Regression and LightGBM model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression</h3>\n",
    "\n",
    "Best results from the baseline model were achieved using binarized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bv = CountVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "train_data_features = bv.fit_transform(X_train)\n",
    "val_data_features = bv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression(C=0.5)\n",
    "logistic.fit(train_data_features.sign(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score at threshold 0.3 is 0.7747866053841103\n",
      "Accuracy score at threshold 0.31 is 0.7754432042022325\n",
      "Accuracy score at threshold 0.32 is 0.7820091923834537\n",
      "Accuracy score at threshold 0.33 is 0.788575180564675\n",
      "Accuracy score at threshold 0.34 is 0.7957977675640184\n",
      "Accuracy score at threshold 0.35 is 0.7971109652002626\n",
      "Accuracy score at threshold 0.36 is 0.7997373604727511\n",
      "Accuracy score at threshold 0.37 is 0.8023637557452397\n",
      "Accuracy score at threshold 0.38 is 0.8003939592908733\n",
      "Accuracy score at threshold 0.39 is 0.8036769533814839\n",
      "Accuracy score at threshold 0.4 is 0.8056467498358503\n",
      "Accuracy score at threshold 0.41 is 0.8063033486539725\n",
      "Accuracy score at threshold 0.42 is 0.8128693368351937\n",
      "Accuracy score at threshold 0.43 is 0.8122127380170716\n",
      "Accuracy score at threshold 0.44 is 0.81483913328956\n",
      "Accuracy score at threshold 0.45 is 0.8181221273801708\n",
      "Accuracy score at threshold 0.46 is 0.814182534471438\n",
      "Accuracy score at threshold 0.47 is 0.8128693368351937\n",
      "Accuracy score at threshold 0.48 is 0.8135259356533159\n",
      "Accuracy score at threshold 0.49 is 0.814182534471438\n"
     ]
    }
   ],
   "source": [
    "logistic_probs = logistic.predict_proba(val_data_features.sign())[:, 1]\n",
    "\n",
    "for thresh in np.arange(0.3, 0.5, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"Accuracy score at threshold {0} is {1}\".format(thresh, accuracy_score(y_val, (logistic_probs>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174655285620486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_probs = logistic.predict_proba(val_data_features)[:, 1]\n",
    "(y_val == (logistic_probs>0.45)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.11109575, 0.40588825, 0.4438563 , ..., 0.69375798, 0.10121786,\n",
       "        0.54541374]),\n",
       " array([0.19468519, 0.48497119, 0.33974634, ..., 0.85849911, 0.33974634,\n",
       "        0.33974634]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_probs, lgb_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score with weight ratio of logistic model:LGB model::0.0:1.0 is 0.7229152987524623\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.1:0.9 is 0.7275114904793172\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.2:0.8 is 0.7426132632961261\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.3:0.7 is 0.7688772160210111\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.4:0.6 is 0.7806959947472094\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.5:0.5 is 0.7951411687458962\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.6:0.4 is 0.8003939592908733\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.7:0.3 is 0.8036769533814839\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.8:0.2 is 0.8108995403808273\n",
      "Accuracy score with weight ratio of logistic model:LGB model::0.9:0.1 is 0.8154957321076822\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0, 1, 0.1):\n",
    "    y_val_probs = (logistic_probs*i) + (lgb_probs*abs(1-i))\n",
    "    print('Accuracy score with weight ratio of logistic model:LGB model::{0}:{1} is {2}'.format(\n",
    "        round(i, 1), round(abs(1-i),1), (y_val==(y_val_probs>0.45)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better off just using the logistic regression model clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-2.0.0",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
